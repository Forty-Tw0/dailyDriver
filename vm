#!/bin/bash

# looking-glass.io - Sharing Keyboard, Monitor, and Mouse via shared memory between host and guest?
# https://wiki.archlinux.org/index.php/PCI_passthrough_via_OVMF

# hyper-v enlightenments make things faster for Window guests. hypervisor=off (-hypervisor) should still hide hyperv
# https://github.com/qemu/qemu/blob/master/docs/hyperv.txt

# I am spoofing SMBIOS HardwareIDs with $RANDOM for ban evasion and fingerprinting mitigation.
# You can find your host's SMBIOS IDs and pass those through instead. Use "dmidecode" to check
# Need to investigate further spoofing of VirtIO drivers and those hardware IDs

# For windows network and storage drivers, load virtio into the cdrom and install the Redhat drivers from there
# Need to spoof these driver IDs later since they aid detection

# ------------------------------------------------------- BIOS Config

# Might need to disable Resizable Base Address Register (BAR) and/or Smart Access Memory (SAM) in your BIOS if you get Code 43 on AMD Vega and later GPUs.

# Ideally, you want every PCI device to be in it's own IOMMU group.
# If your BIOS has Access Control Services (ACS), enable it to split up the groups. Mine BIOS was hiding this option under PCI Advanced Error Reporting (AER).
# If your groups are still not granular enough, try the ACS Override kernel patch that will fake group isolation well enough (insecure!) for this to work.

# ------------------------------------------------------- Kernel Config

# Outdated? https://wiki.installgentoo.com/index.php/PCI_passthrough#Step_0:_Compile_IOMMU_support_if_you_use_Gento
# Recompile kernel with these flags: https://wiki.installgentoo.com/index.php/PCI_passthrough

# Device Drivers --->
#  <M> VFIO Non-Privileged userspace driver framework  ---
#  [*]   VFIO No-IOMMU support  ----
#  <M>   VFIO support for PCI devices
#  [*]     VFIO PCI support for VGA devices
#  [*]     VFIO PCI extensions for Intel graphics (GVT-d)
#  <M>   Mediated device driver framework
#  <M>     VFIO driver for Mediated devices

#  [*] IOMMU Hardware Support --->
#        Generic IOMMU Pagetable Support  ----
#  [ ]   Export IOMMU internals in DebugFS
#  [ ]   IOMMU passthrough by default
#  [*]   AMD IOMMU support
#  <*>     AMD IOMMU Version 2 driver
#  [*]   Support for Intel IOMMU using DMA Remapping Devices
#  [*]     Support for Shared Virtual Memory with Intel IOMMU
#  [*]     Enable Intel DMA Remapping Devices by default
#  [*]     Enable Intel IOMMU scalable mode by default
#  [*]   Support for Interrupt Remapping

# > Networking support > Networking options > Network packet filtering framework (Netfilter) > IP: Netfilter Configuration
#  <M>   iptables NAT support
#  <M>     MASQUERADE target support

# ACS kernal patch to separate IOMMU groups, if your BIOS doesn't have an ACS option this might help
#curl -L https://github.com/feniksa/gentoo_ACS_override_patch/archive/master.zip -o /tmp/master.zip
#unzip /tmp/master.zip -d /tmp
#sudo cp -r /tmp/gentoo_ACS_override_patch-master/sys-kernel /etc/portage/patches/
# add pcie_acs_override=downstream to the GRUB_CMDLINE_LINUX in /etc/default/grub

# Make sure modesetting and the framebuffer don't reserve the GPU, enable IOMMU
# /etc/default/grub GRUB_CMDLINE_LINUX="video=efifb:off,vesafb:off iommu=pt intel_iommu=on amd_iommu=force_isolation"

# ------------------------------------------------------- Code 43 on GPU in Windows

# Some GPUs might want their ROM to be passed in, I think Nvidea drivers do some checks for this. Maybe it isn't needed if the hypervisor is undetected.
# Nvidea drivers will intentionally refuse to load properly if they detect the hv_vendor_id to be the default of "Microsoft Hv". (code 43 driver error)
# You might be able to set hv_vendor_id to something else on the -cpu args (I have it set to random), but you might also need to pass through the ROM.
# Dump ROM for passthorugh, you might need to put your GPU in PCIeslot2 and have another GPU in slot1 while booted into a live CD for this to work!
#echo 1 > /sys/bus/pci/devices/$VGA_DEVICE/rom
#cat /sys/bus/pci/devices/$VGA_DEVICE/rom > /tmp/gpu.rom
#echo 0 > /sys/bus/pci/devices/$VGA_DEVICE/rom
# add the rom in the qemu flags
#-device vfio-pci,host=$VGA_DEVICE,multifunction=on,x-vga=on,rombar=1,romfile=./gpu.rom \

# AMD "Reset bug" might prevent the guest from rebooting, because the GPU state can't be reset. Try to emerge with app-emulation/vendor-reset

# ------------------------------------------------------- PCI GPU ID

# Configure PCI device IDs and driver for GPU. The entire IOMMU group must be unbound from the host the following command can dump those groupings.
# for iommu_group in $(find /sys/kernel/iommu_groups/ -maxdepth 1 -mindepth 1 -type d | sed 's/\/sys\/kernel\/iommu_groups\///g' | sort -n); do echo "IOMMU group $(basename "$iommu_group")"; for device in $(ls -1 "/sys/kernel/iommu_groups/$iommu_group"/devices/); do echo -n $'\t'; lspci -nns "$device"; done; done
#VGA_DRIVER=nouveau
VGA_DRIVER=amdgpu
VGA_DEVICE=0000:03:00.0
HDMI_AUDIO_DEVICE=0000:03:00.1 # you might not have to bind the multifunction audio to vfio, but it probably has to be unbound from the linux driver

# ------------------------------------------------------- Start scripting things

echo "Refresh the sudo timeout to avoid a race condition"
sudo -v

# Load VFIO-PCI driver so that /dev/vfio-pci is present
sudo modprobe vfio-pci

# Unbinds a device from the current driver and rebinds to the given driver.
changeDriver() {
	dev="$1"
	driver="$2"

	if [ -z $driver ] | [ -z $dev ];
	then
		return 1
	fi

	vendor=$(cat /sys/bus/pci/devices/$dev/vendor)
	device=$(cat /sys/bus/pci/devices/$dev/device)

	echo -n Unbinding $vendor:$device
	if [ -e /sys/bus/pci/devices/$dev/driver ]; then
		echo $dev | sudo tee /sys/bus/pci/devices/$dev/driver/unbind > /dev/null
		while [ -e /sys/bus/pci/devices/$dev/driver ]; do
			sleep 0.5
		done
	fi
	echo " OK!"

	echo -n Binding \'$driver\' to $vendor:$device
	echo $vendor $device | sudo tee /sys/bus/pci/drivers/$driver/new_id > /dev/null
	echo $dev | sudo tee /sys/bus/pci/drivers/$driver/bind > /dev/null
	echo " OK!"

	return 0
}

# Save current X config to restore it later
#savedXconfig=$(while read -r line ; do printf " %s" $line; done <<< `xrandr --current | grep " connected" | awk '{split($3,sizepos,"+"); print "--output "$1" --mode "sizepos[1]" --pos "sizepos[2]"x"sizepos[3]}'`)
#echo "Saved X config as \'xrandr $savedXconfig\'"
# Detatch from X, assuming the first monitor is the primary host monitor
#for monitor in `xrandr --listactivemonitors | awk 'NR>2 {print $4}'`; do xrandr --output $monitor --off; done

# If the host was booted with the GPU in use by the frame buffer, unbind these
#echo "Unbinding virtual consoles and efifb"
#echo 0 | sudo tee /sys/class/vtconsole/vtcon0/bind
#echo 0 | sudo tee /sys/class/vtconsole/vtcon1/bind
#echo efi-framebuffer.0 | sudo tee /sys/bus/platform/drivers/efi-framebuffer/unbind
# or unbind the GPU before X is started

# Unbind GPU and its audio device.
echo "Unbinding GPU"
changeDriver $VGA_DEVICE vfio-pci
changeDriver $HDMI_AUDIO_DEVICE vfio-pci

### Networking
sudo modprobe vhost_net
sudo modprobe tun
sudo tunctl -t vmtap10
sudo ip link set dev vmtap10 address 42:42:42:42:42:42 # this MAC is only visible to the host
sudo ifconfig vmtap10 192.168.42.1 up # the VM can't see this IP either
echo 1 | sudo tee /proc/sys/net/ipv4/ip_forward > /dev/null
sudo iptables -t nat -A POSTROUTING -o eno1 -j MASQUERADE # change this NIC name if needed
sudo iptables -A FORWARD -i vmtap10 -j ACCEPT
sudo iptables -A FORWARD -o vmtap10 -m state --state RELATED,ESTABLISHED -j ACCEPT

### Memory Allocation
RAM=24 # in gigabytes
#echo Mounting and dynamically allocating ${RAM}G of huge pages
# Try to drop and compact memory for contiguous blocks
#echo 3 | sudo tee /proc/sys/vm/drop_caches
#echo 1 | sudo tee /proc/sys/vm/compact_memory
#sudo mkdir -p /dev/hugepages
#sudo mountpoint -q /dev/hugepages && sudo mount -t hugetlbfs hugetlbfs /hugepages
# 1G pages are hard to get, so lets just use the standard 2M since they are better than the normal 4K pages
#sudo sysctl vm.nr_hugepages=$(($RAM/2*1024)) # number of 2M pages
# Huge pages are hard to allocate when RAM is fragmented
#-mem-path /dev/hugepages \

### CPU Affinity and Thread Priority
thread_count=16 # number of threads to give the VM, treat this as CPU cores with 1 thread each
# Spawn a background bash script to wait until VM threads have been created by Qemu
{
  ### Notes on blocked sigmasks for Qemu threads
  # 0000000010002240 main thread
  #   These signals are blocked in qemu/util/main-loop.c qemu_signal_init()
  #   7=SIGBUS
  #   10=SIG_IPI==SIGUSR1 defined in qemu/include/qemu/main-loop.h
  #   14=SIGALRM
  #   29=SIGIO
  # fffffffe7ffbfa77 worker threads that come and go
  #   These signals are unblocked by qemu/util/qemu-thread-posix.c qemu_thread_create()
  #   4=SIGILL
  #   7=SIGBUS ----
  #   8=SIGFPE
  #   9=SIGKILL
  #   10=SIG_IPI=SIGUSR1 ----
  #   11=SIGSEGV
  #   19=SIGSTOP
  #   32=SIGRTMIN
  #   33=SIGRTMIN+1
  # fffffffe7ffbf837 actual VM threads that we care about
  #   These 2 signals may be unblocked by qemu/accel/kvm/kvm-all.c kvm_init_cpu_signals()
  #   SIGBUS only gets unblocked if KVM_HAVE_MCE_INJECTION is defined
  #   SIG_IPI always gets unblocked and always exists on the main thread, this is a good indicator
  #     also gets unblocked by qemu/target/i386/hvf/hvf.c hvf_init_vcpu() but we don't use accel=hvf
  # In short:
  #   We can identify the important VM threads by using the sigmask of fffffffe7ffbf837 because it has SIG_IPI/SIGUSR1
  echo "Waiting for QEMU guest threads"
  while [ $(ps -Leo sigmask,comm | grep "fffffffe7ffbf837 qemu" | wc -l) -lt $thread_count ]; do
    sleep 0.5
  done
  echo "Identified QEMU guest threads"
  tnum=0
  for pid in `ps -Leo tid,sigmask,comm | grep "fffffffe7ffbf837 qemu" | awk '{print $1}'`; do
    # TODO: Make this topology aware (NUMA nodes) Intel hyperthreading and AMD SMT do this differently
    sudo taskset -pc $((tnum++)) $pid
    # Linux has priorities of 0-99 reserved for realtime scheduling and 100-139 for normal threads
    # Set the niceness to -19 so that they have the second highest priority of 1 (which is 101 in userspace)
    sudo renice -n -19 -p $pid
    # We could give these threads the highest realtime priority (99 being 0 on the above scale), but that AGGRESSIVELY preempts the host
    # --fifo will wait until the thread releases control (if ever!), the thread will preempt others whenever it wants
    # --rr preempts like FIFO but the thread are required to yield after their time slice ends
    #sudo chrt --rr -p 99 $pid
  done
  echo "QEMU guest threads pinned to CPU cores and prioritised"
} &

### QEMU
sudo qemu-system-x86_64 -monitor stdio -vnc :1 -vga none -usb \
-machine type=q35,accel=kvm,kernel_irqchip=on -enable-kvm \
-rtc base=localtime -no-hpet \
-drive if=pflash,readonly=on,format=raw,file=/usr/share/edk2-ovmf/OVMF_CODE.fd \
-drive if=pflash,format=raw,file=/usr/share/edk2-ovmf/OVMF_VARS.fd \
-drive file=/dev/nvme1n1,format=raw,cache=none,aio=native \
-smbios type=0,vendor=$RANDOM,version=$RANDOM,date=$RANDOM,release=$RANDOM.$RANDOM,uefi=on \
-smbios type=1,manufacturer=$RANDOM,product=$RANDOM,version=$RANDOM,serial=$RANDOM,uuid=$(uuidgen),sku=$RANDOM,family=$RANDOM \
-smbios type=2,manufacturer=$RANDOM,product=$RANDOM,version=$RANDOM,serial=$RANDOM,asset=$RANDOM,location=$RANDOM \
-smbios type=3,manufacturer=$RANDOM,version=$RANDOM,serial=$RANDOM,asset=$RANDOM,sku=$RANDOM \
-smbios type=4,manufacturer=$RANDOM,sock_pfx=$RANDOM,version=$RANDOM,serial=$RANDOM,asset=$RANDOM,part=$RANDOM \
-smbios type=11,value=$RANDOM,value=$RANDOM,value=$RANDOM,value=$RANDOM \
-smbios type=17,manufacturer=$RANDOM,serial=$RANDOM,asset=$RANDOM,loc_pfx=$RANDOM,bank=$RANDOM,part=$RANDOM,speed=$RANDOM \
-cpu host,kvm=off,hypervisor=false,-hypervisor,hv-relaxed,hv-vapic,hv_spinlocks=0x1fff,hv-vpindex,hv-runtime,hv-time,hv-synic,hv-stimer,hv-tlbflush,hv-ipi,hv_vendor_id=$RANDOM \
-smp sockets=1,cores=$thread_count,threads=1 -m ${RAM}G \
-device pcie-root-port,id=gpubus,chassis=1 \
-device vfio-pci,host=$VGA_DEVICE,bus=gpubus,addr=00.0,multifunction=on,x-vga=on \
-device vfio-pci,host=$HDMI_AUDIO_DEVICE,bus=gpubus,addr=00.1 \
-device ich9-intel-hda,bus=pcie.0,addr=1b.0 -device hda-micro,audiodev=hda \
-device usb-audio,audiodev=usb,multi=on \
-audiodev pa,id=hda,server=unix:/run/user/$(id -u)/pulse/native,timer-period=1000,in.frequency=48000,out.frequency=48000 \
-audiodev pa,id=usb,server=unix:/run/user/$(id -u)/pulse/native,out.mixing-engine=off \
-device usb-host,hostbus=3,hostport=2 \
-device usb-host,hostbus=3,hostport=7 \
-device usb-host,vendorid=0x046d,productid=0xc215 \
-device usb-host,vendorid=0x045e,productid=0x028e \
-netdev tap,id=tap0,ifname=vmtap10,script=no,downscript=no,vhost=on \
-device virtio-net-pci,netdev=tap0,mac=$(openssl rand -hex 6 | sed 's/\(..\)/\1:/g; s/:$//') \
-netdev user,id=samba,smb=/samba -device virtio-net-pci,netdev=samba

# todo: nvme controller passthrough https://qemu.readthedocs.io/en/latest/system/qemu-block-drivers.html

# USB bus, port, vendorID:productID, name
# find /sys/bus/usb/devices/ | xargs -I{} cat {}/busnum {}/devpath {}/idVendor {}/idProduct {}/product 2> /dev/null | sed 'N;N;N;N;s/\n/ /g' | awk '{printf $1 "-" $2 "\t" $3 ":" $4 "\t "; for(i=5;i<=NF;i++){printf $i}; printf "\n"}' | sort

# ------------------------------------------------------- Cleanup

echo "Exited QEMU, starting cleanup"
echo "Refresh the sudo timeout to avoid a race condition"
sudo -v

#echo "Freeing huge pages"
#sudo sysctl vm.nr_hugepages=0

echo "Cleaning up the virtual network"
sudo iptables -F
sudo iptables -t nat -F POSTROUTING
sudo ip link delete vmtap10
sudo tunctl -d vmtap10

echo "Rebinding GPU"
#changeDriver $VGA_DEVICE $VGA_DRIVER
#changeDriver $HDMI_AUDIO_DEVICE $AUDIO_DRIVER

#echo "Rebinding efifb and virtual consoles # so that the host can use the GPU again"
#echo efi-framebuffer.0 | sudo tee /sys/bus/platform/drivers/efi-framebuffer/bind
#echo 1 | sudo tee /sys/class/vtconsole/vtcon0/bind
#echo 1 | sudo tee /sys/class/vtconsole/vtcon1/bind

# re-enable monitors in X
#xrandr $savedXconfig

echo "Done"
